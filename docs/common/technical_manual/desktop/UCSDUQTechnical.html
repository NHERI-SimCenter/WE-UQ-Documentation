

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Methods in UCSD UQ Engine &mdash; Wind Engineering with Uncertainty Quantification  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=66b6668b" />
      <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css?v=a5c4661c" />
      <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../../_static/sphinxcontrib-images/LightBox2/lightbox2/dist/css/lightbox.css?v=5c84f910" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css?v=4b5bf716" />

  
      <script src="../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../_static/design-tabs.js?v=36754332"></script>
      <script src="../../../_static/sphinxcontrib-images/LightBox2/lightbox2/dist/js/lightbox-plus-jquery.min.js?v=ffc8af2d"></script>
      <script src="../../../_static/sphinxcontrib-images/LightBox2/lightbox2-customize/jquery-noconflict.js?v=12818e64"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
<script src="https://cdn.jsdelivr.net/npm/vega@5.12.1"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-lite@4.13.1"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-embed@6.8.0"></script>
<style media="screen">.vega-actions a {margin-right: 5px;}</style>
<link href="../../../_static/css/bootstrap.css" rel="stylesheet">

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #F2F2F2" >
<a href="https://simcenter.designsafe-ci.org/" style="margin-bottom: 0px;">
  <img src="../../../_static/img/SimCenter-Only.png" class="logo" alt="Org-Logo" />
</a>
<hr style="margin: 0px;">

  <a href="../../../index.html" class="icon icon-home"> Wind Engineering with Uncertainty Quantification



</a>




<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>


        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/about/WEUQ/about.html">1. About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../front-matter/desktop/ack.html">2. Acknowledgments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../front-matter/license.html">3. Copyright and License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../front-matter/cite.html">4. How To Cite</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/releases/weCapabilities.html">5. Capabilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/releases/weReleaseNotes.html">6. Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/releases/wePlans.html">7. Release Plans</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../front-matter/glossary.html">8. Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../front-matter/abbreviations.html">9. Abbreviations</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Manual</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/installation/desktop/installation.html">1. Running Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/usage/desktop/usage.html">2. User Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/usage/desktop/wind/tools.html">3. Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/examples/desktop/examples.html">4. Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/usage/desktop/jobs.html">5. Jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/troubleshooting/desktop/troubleshooting.html">6. Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reqments/WEUQ.html">7. Requirements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../user_manual/bugs.html">8. Bugs &amp; Feature Requests</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Technical Manual</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="uqTechnical.html">1. Dakota Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="damping.html">2. Damping Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="WEUQ/scalingWindTunnelData.html">3. Scaling Wind Tunnel Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="WEUQ/TinF.html">4. Turbulence Inflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="WEUQ/dataInformedStochasticWind.html">5. Wind-tunnel informed stochastic wind load</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Manual</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../developer_manual/how_to_build/desktop/how_to_build.html">1. How to Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_manual/how_to_extend.html">2. How to Extend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_manual/verification/desktop/verification.html">3. Verification</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #F2F2F2" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Wind Engineering with Uncertainty Quantification</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Methods in UCSD UQ Engine</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/common/technical_manual/desktop/UCSDUQTechnical.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="methods-in-ucsd-uq-engine">
<span id="lbluqucsdsimtechnical"></span><h1>Methods in UCSD UQ Engine<a class="headerlink" href="#methods-in-ucsd-uq-engine" title="Link to this heading"></a></h1>
<section id="transitional-markov-chain-monte-carlo">
<span id="lbluqucsd-tmcmc"></span><h2>Transitional Markov chain Monte Carlo<a class="headerlink" href="#transitional-markov-chain-monte-carlo" title="Link to this heading"></a></h2>
<p>TMCMC is a numerical method used to obtain samples of the target posterior PDF. This algorithm is flexible, applicable in general settings, and parallelizable. Thus, it can be used to effectively sample the posterior PDF, when the likelihood function involves an FE model evaluation, using high-performance computing (HPC) resources.</p>
<p>In Bayesian inference, the posterior probability distribution of the unknown quantities, represented by the vector <span class="math notranslate nohighlight">\(\mathbf{\theta}\)</span>, is obtained by applying Bayes’ rule as follows:</p>
<div class="math notranslate nohighlight" id="equation-common-technical-manual-desktop-ucsduqtechnical-0">
<span class="eqno">()<a class="headerlink" href="#equation-common-technical-manual-desktop-ucsduqtechnical-0" title="Link to this equation"></a></span>\[p (\mathbf{\theta \ | \ y}) = \frac{p (\mathbf{y \ | \ \theta}) \times p(\mathbf{\theta})}{\int p (\mathbf{y \ | \ \theta}) \times p(\mathbf{\theta}) \ d\mathbf{\theta}}\]</div>
<p>The idea behind TMCMC is to avoid sampling directly from the target posterior PDF <span class="math notranslate nohighlight">\(p(\mathbf{\theta \ | \ y})\)</span>,
but to sample from a series of simpler intermediate probability distributions that converge to the target posterior PDF. To achieve this, the TMCMC sampler proceeds through a series of stages, starting from the prior PDF until the posterior PDF. These intermediate probability distributions (called tempered posterior PDFs) are controlled by the tempering parameter <span class="math notranslate nohighlight">\(\beta_j\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-common-technical-manual-desktop-ucsduqtechnical-1">
<span class="eqno">()<a class="headerlink" href="#equation-common-technical-manual-desktop-ucsduqtechnical-1" title="Link to this equation"></a></span>\[p(\theta|y)_j \propto p(y | \theta)^{\beta_j} \times p(\theta); \quad j = 0, 1, \ldots, m; \quad 0 = \beta_0 &lt; \beta_1 &lt; \ldots &lt; \beta_m = 1\]</div>
<p>Index <span class="math notranslate nohighlight">\(j\)</span> denotes the stage number, <span class="math notranslate nohighlight">\(m\)</span> denotes the total number of stages, and <span class="math notranslate nohighlight">\(p (\mathbf{\theta \ | \ y})_j\)</span> is the tempered posterior PDF at stage <span class="math notranslate nohighlight">\(j\)</span> controlled by the parameter <span class="math notranslate nohighlight">\(\beta_j\)</span>. At the initial stage <span class="math notranslate nohighlight">\((j = 0)\)</span>, parameter <span class="math notranslate nohighlight">\(\beta_0 = 0\)</span>, the tempered distribution <span class="math notranslate nohighlight">\(p(\mathbf{\theta \ | \ y})_{j=0}\)</span> is just the
prior joint PDF <span class="math notranslate nohighlight">\(p(\theta)\)</span>. The TMCMC sampler progresses by monotonically increasing the value of <span class="math notranslate nohighlight">\(\beta_j\)</span>, at each stage <span class="math notranslate nohighlight">\(j\)</span>, until it reaches the value of 1. At the final stage <span class="math notranslate nohighlight">\((j = m)\)</span>, parameter <span class="math notranslate nohighlight">\(\beta_m = 1\)</span>, the tempered distribution <span class="math notranslate nohighlight">\(p(\mathbf{\theta \ | \ y})_{j = m}\)</span> is the target posterior joint PDF <span class="math notranslate nohighlight">\(p(\mathbf{\theta \ | \ y})\)</span>.</p>
<p>TMCMC represents the tempered posterior PDF at every stage by a set of weighted samples (known as particles). TMCMC approximates the <span class="math notranslate nohighlight">\(j^{th}\)</span> stage tempered posterior PDF <span class="math notranslate nohighlight">\(p(\mathbf{\theta \ | \ y})_j\)</span>  by weighing, resampling, and perturbing the particles of the <span class="math notranslate nohighlight">\(j-1^{th}\)</span> stage intermediate joint PDF <span class="math notranslate nohighlight">\(p(\mathbf{\theta \ | \ y})_{j-1}\)</span>. For details about the TMCMC algorithm, the interested reader is referred to Ching and Chen <a class="reference internal" href="#ching2007" id="id1"><span>[Ching2007]</span></a>, Minson et. al. <a class="reference internal" href="#minson2013" id="id2"><span>[Minson2013]</span></a>.</p>
<div role="list" class="citation-list">
<div class="citation" id="ching2007" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">Ching2007</a><span class="fn-bracket">]</span></span>
<ol class="upperalpha simple" start="10">
<li><p>Ching and Y.-C. Chen, “Transitional Markov Chain Monte Carlo Method for Bayesian Model Updating, Model Class Selection, and Model Averaging”, <em>Journal of Engineering Mechanics</em>, 133(7), 816-832, 2007.</p></li>
</ol>
</div>
<div class="citation" id="minson2013" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">Minson2013</a><span class="fn-bracket">]</span></span>
<ol class="upperalpha simple" start="19">
<li><ol class="upperalpha simple" start="5">
<li><p>Minson, M. Simons, and J. L. Beck, “Bayesian Inversion for Finite Fault Earthquake Source Models I-Theory and Algorithm”, <em>Geophysical Journal International</em>, 194(3), 1701- 1726, 2013.</p></li>
</ol>
</li>
</ol>
</div>
</div>
</section>
<section id="gaussian-process-aided-bayesian-calibration-gp-ab">
<span id="lbluqucsd-ugpab2"></span><h2>Gaussian Process-Aided Bayesian Calibration (GP-AB)<a class="headerlink" href="#gaussian-process-aided-bayesian-calibration-gp-ab" title="Link to this heading"></a></h2>
<p>The GP-AB algorithm <a class="reference internal" href="#taflanidis2025" id="id3"><span>[Taflanidis2025]</span></a> is a surrogate-aided extension of the Transitional Markov Chain Monte Carlo
(<a class="reference internal" href="#lbluqucsd-tmcmc"><span class="std std-ref">Transitional Markov chain Monte Carlo</span></a>) sampling method. It is designed for Bayesian updating problems
in which evaluating the likelihood function <span class="math notranslate nohighlight">\(L_D(\theta, q)\)</span> requires computationally expensive
finite element (FE) simulations. By replacing these simulations with predictions from a
Gaussian Process (GP) surrogate model, GP-AB can significantly reduce the computational cost of posterior sampling
while still converging to the same posterior distribution obtained if the original FE model was used.</p>
<p>In GP-AB, the GP surrogate is developed and <strong>adaptively refined</strong> in an iterative process using a
design of experiments (DoE) strategy that balances <em>exploitation</em> (focusing on regions of high
posterior probability) and <em>exploration</em> (sampling previously under-explored regions). This
adaptive refinement continues until the surrogate-based posterior converges according to specified
criteria described in the following sections.</p>
<section id="formulation-of-the-bayesian-calibration-problem">
<h3>Formulation of the Bayesian Calibration Problem<a class="headerlink" href="#formulation-of-the-bayesian-calibration-problem" title="Link to this heading"></a></h3>
<p>Consider a model with unknown parameters <span class="math notranslate nohighlight">\(\theta \in \mathbb{R}^{n_\theta}\)</span> and
prediction error parameters <span class="math notranslate nohighlight">\(q \in \mathbb{R}^{n_q}\)</span>, where <span class="math notranslate nohighlight">\(n_\theta\)</span> and
<span class="math notranslate nohighlight">\(n_q\)</span> denote their respective dimensions. Let <span class="math notranslate nohighlight">\(D\)</span> denote the available
input-output data, consisting of measured system outputs
<span class="math notranslate nohighlight">\(\hat{\mathbf{y}}\)</span> for known inputs <span class="math notranslate nohighlight">\(\hat{\mathbf{u}}\)</span>.</p>
<p>The posterior probability density function (PDF) is given by Bayes’ rule:</p>
<div class="math notranslate nohighlight" id="equation-ugpab2-bayes-rule">
<span class="eqno">()<a class="headerlink" href="#equation-ugpab2-bayes-rule" title="Link to this equation"></a></span>\[p(\theta, q \,|\, D) \ \propto\ L_D(\theta, q) \ p(\theta) \ p(q)\]</div>
<p>where <span class="math notranslate nohighlight">\(L_D(\theta, q)\)</span> is the likelihood of the data given the parameters,
and <span class="math notranslate nohighlight">\(p(\theta)\)</span> and <span class="math notranslate nohighlight">\(p(q)\)</span> are the prior PDFs.</p>
<p>The target posterior <span class="math notranslate nohighlight">\(p(\theta, q \,|\, D)\)</span> is approximated using TMCMC, but
instead of evaluating the expensive model response <span class="math notranslate nohighlight">\(\mathbf{y}(\theta|\mathbf{u})\)</span>
at every sample, GP-AB builds a GP surrogate that predicts this response accurately
in the regions of interest.</p>
</section>
<section id="likelihood-function">
<h3>Likelihood Function<a class="headerlink" href="#likelihood-function" title="Link to this heading"></a></h3>
<p>In the quoFEM implementation, a Gaussian likelihood is used, with unknown error variances.
Assigning a Jeffreys prior <span class="math notranslate nohighlight">\(p(q) \propto 1/q\)</span> to the unknown variance
yields a closed-form likelihood in terms of the model parameters <span class="math notranslate nohighlight">\(\theta\)</span> only.</p>
<p>Let <span class="math notranslate nohighlight">\(z(\theta) \in \mathbb{R}^{n_z}\)</span> be the stacked vector of all model outputs
for the observation cases in <span class="math notranslate nohighlight">\(D\)</span>, and <span class="math notranslate nohighlight">\(\hat{z}\)</span> the corresponding stacked
measurements. The residual vector is:</p>
<div class="math notranslate nohighlight" id="equation-common-technical-manual-desktop-ucsduqtechnical-2">
<span class="eqno">()<a class="headerlink" href="#equation-common-technical-manual-desktop-ucsduqtechnical-2" title="Link to this equation"></a></span>\[\mathbf{r}(\theta) = \hat{z} - z(\theta)\]</div>
<p>The resulting marginalized likelihood is:</p>
<div class="math notranslate nohighlight" id="equation-common-technical-manual-desktop-ucsduqtechnical-3">
<span class="eqno">()<a class="headerlink" href="#equation-common-technical-manual-desktop-ucsduqtechnical-3" title="Link to this equation"></a></span>\[L_D(\theta) \ \propto \
\left[ \mathbf{r}(\theta)^\mathsf{T} \mathbf{r}(\theta) \right]^{-\,n_D/2}\]</div>
<p>where <span class="math notranslate nohighlight">\(n_D = n_z\)</span> is the total number of scalar observations.</p>
</section>
<section id="surrogate-model-formulation">
<h3>Surrogate Model Formulation<a class="headerlink" href="#surrogate-model-formulation" title="Link to this heading"></a></h3>
<p>The surrogate is built for the vectorized model response:</p>
<div class="math notranslate nohighlight" id="equation-common-technical-manual-desktop-ucsduqtechnical-4">
<span class="eqno">()<a class="headerlink" href="#equation-common-technical-manual-desktop-ucsduqtechnical-4" title="Link to this equation"></a></span>\[z(\theta) = \mathrm{vec}\!\left( Y_D(\theta) \right) \in \mathbb{R}^{n_z}\]</div>
<p>where <span class="math notranslate nohighlight">\(Y_D(\theta)\)</span> stacks the model outputs for all observation cases in <span class="math notranslate nohighlight">\(D\)</span>,
and <span class="math notranslate nohighlight">\(n_z = n_y \, n_D\)</span> is the total number of output components.</p>
<p>Because <span class="math notranslate nohighlight">\(n_z\)</span> can be large, <strong>Principal Component Analysis (PCA)</strong> is used to
reduce the output dimension before GP training. Let <span class="math notranslate nohighlight">\(\{ v_q \}_{q=1}^{n_v}\)</span>
be the retained principal component (PC) scores, chosen so that a prescribed fraction
<span class="math notranslate nohighlight">\(r_{pc}\)</span> of the variance in the training outputs is preserved. A separate GP
metamodel is calibrated for each PC score:</p>
<div class="math notranslate nohighlight" id="equation-common-technical-manual-desktop-ucsduqtechnical-5">
<span class="eqno">()<a class="headerlink" href="#equation-common-technical-manual-desktop-ucsduqtechnical-5" title="Link to this equation"></a></span>\[v_q(\theta) \ \sim \ GP\!\left( \tilde{v}_q(\theta), \ \sigma_{v_q}^2(\theta) \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{v}_q(\theta)\)</span> and <span class="math notranslate nohighlight">\(\sigma_{v_q}^2(\theta)\)</span> are the GP-predicted mean
and variance, respectively. The predicted model response is reconstructed as:</p>
<div class="math notranslate nohighlight" id="equation-common-technical-manual-desktop-ucsduqtechnical-6">
<span class="eqno">()<a class="headerlink" href="#equation-common-technical-manual-desktop-ucsduqtechnical-6" title="Link to this equation"></a></span>\[\tilde{z}(\theta) \ =\ \mu_z + P \, \tilde{\mathbf{v}}(\theta)\]</div>
<p>with <span class="math notranslate nohighlight">\(\mu_z\)</span> the mean output vector from training data, <span class="math notranslate nohighlight">\(P\)</span> the PCA projection
matrix, and <span class="math notranslate nohighlight">\(\tilde{\mathbf{v}}(\theta)\)</span> the vector of GP-predicted PC scores.</p>
<p>The GP-predicted response <span class="math notranslate nohighlight">\(\tilde{z}(\theta)\)</span> is then used in place of
<span class="math notranslate nohighlight">\(z(\theta)\)</span> when evaluating the likelihood and posterior.</p>
</section>
<section id="adaptive-design-of-experiments-doe">
<h3>Adaptive Design of Experiments (DoE)<a class="headerlink" href="#adaptive-design-of-experiments-doe" title="Link to this heading"></a></h3>
<p>At each iteration <span class="math notranslate nohighlight">\(k\)</span>, the surrogate is improved by adding <span class="math notranslate nohighlight">\(2 * {n_\theta}\)</span> new
training points. Candidate points are chosen using the <strong>weighted integrated mean
squared error (IMSE)</strong> acquisition function:</p>
<div class="math notranslate nohighlight" id="equation-ugpab2-imse">
<span class="eqno">()<a class="headerlink" href="#equation-ugpab2-imse" title="Link to this equation"></a></span>\[\mathrm{IMSE}(\theta_\text{new}) =
\int_{\Theta_d} \phi(\theta) \ \hat{\sigma}^2\!\left( \theta \,\middle|\, \Theta^{(k)}, \theta_\text{new}, s^{*(k)} \right) \, d\theta\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Theta_d\)</span> is the parameter domain,</p></li>
<li><p><span class="math notranslate nohighlight">\(\phi(\theta)\)</span> is a weight function prioritizing important regions,</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> is the average GP predictive variance across all output components
after adding <span class="math notranslate nohighlight">\(\theta_\text{new}\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(s^{*(k)}\)</span> are the optimized GP hyperparameters at iteration <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
</ul>
<p><strong>Weight function:</strong>
<span class="math notranslate nohighlight">\(\phi(\theta)\)</span> is a convex combination of the GP-approximated <em>target</em> posterior and some <em>intermediate</em> TMCMC densities from the current iteration:</p>
<div class="math notranslate nohighlight" id="equation-common-technical-manual-desktop-ucsduqtechnical-7">
<span class="eqno">()<a class="headerlink" href="#equation-common-technical-manual-desktop-ucsduqtechnical-7" title="Link to this equation"></a></span>\[\phi(\theta) = \sum_{j={j^*}}^{j_t} \tau_j \ \tilde{\pi}^{(k)}_{[j]}(\theta),
\quad \sum_j \tau_j = 1\]</div>
<p>This balances exploitation of the high-probability posterior regions with exploration
along the TMCMC path from prior to posterior. <span class="math notranslate nohighlight">\(j^*\)</span> is defined in the following section on warm-starting TMCMC, and <span class="math notranslate nohighlight">\(j_t\)</span> is the total number of stages in the current iteration.</p>
<p>A fraction of the new points is selected
purely for exploration by setting <span class="math notranslate nohighlight">\(\phi(\theta) \equiv 1\)</span>.</p>
</section>
<section id="convergence-assessment">
<h3>Convergence Assessment<a class="headerlink" href="#convergence-assessment" title="Link to this heading"></a></h3>
<p>Convergence of the surrogate-based posterior is tested between consecutive iterations
using a <strong>dimension-normalized KL divergence:</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight" id="equation-ugpab2-gkl">
<span class="eqno">()<a class="headerlink" href="#equation-ugpab2-gkl" title="Link to this equation"></a></span>\[g^{(k)}_{\mathrm{KL}} =
\frac{1}{n_\theta} \ \mathrm{KL}\!\left( \tilde{\pi}^{(k)}(\theta) \ \big\| \ \tilde{\pi}^{(k-1)}(\theta) \right)\]</div>
<p>where the KL divergence is estimated by importance sampling using the TMCMC samples.</p>
</div></blockquote>
<p>An optional <strong>LOOCV error metric</strong> <span class="math notranslate nohighlight">\(g^{(k)}_{\mathrm{CV}}\)</span> can be used to assess
GP predictive accuracy independently of the posterior approximation.</p>
<p>If <span class="math notranslate nohighlight">\(g_{\mathrm{KL}} &lt; c_{\mathrm{KL}}\)</span> and <span class="math notranslate nohighlight">\(g_{\mathrm{CV}} &lt; c_{\mathrm{CV}}\)</span>,
the algorithm stops.</p>
</section>
<section id="warm-start-tmcmc">
<h3>Warm-Start TMCMC<a class="headerlink" href="#warm-start-tmcmc" title="Link to this heading"></a></h3>
<p>If the LOOCV error <span class="math notranslate nohighlight">\(g_{\mathrm{CV}}\)</span> is below a threshold, TMCMC can be <em>warm-started</em>
at a later stage <span class="math notranslate nohighlight">\(j^*\)</span> by reweighting samples from the same stage of the previous
iteration, avoiding re-sampling from the prior. The stage <span class="math notranslate nohighlight">\(j^*\)</span> is chosen such that
the coefficient of variation of these weights is below a specified limit.</p>
</section>
<section id="algorithm-summary">
<h3>Algorithm Summary<a class="headerlink" href="#algorithm-summary" title="Link to this heading"></a></h3>
<p>The GP-AB algorithm proceeds as follows:</p>
<ol class="arabic simple">
<li><p><strong>Initialization:</strong>
- Select initial training points in <span class="math notranslate nohighlight">\(\Theta_d\)</span> via a space-filling design.
- Evaluate the high-fidelity model at these points and build the initial PCA+GP surrogate.</p></li>
<li><p><strong>Posterior Approximation:</strong>
- Use the GP surrogate to evaluate <span class="math notranslate nohighlight">\(\tilde{L}^{(k)}_D(\theta, q)\)</span> and run TMCMC (warm-start if possible) to sample the posterior.</p></li>
<li><p><strong>Convergence Check:</strong>
- Compute <span class="math notranslate nohighlight">\(g_{\mathrm{KL}}\)</span>.
- If it is below a threshold (0.001), stop.</p></li>
<li><p><strong>Adaptive DoE:</strong>
- Select <span class="math notranslate nohighlight">\(\lceil r_{\mathrm{ex}} * {2*n_\theta} \rceil\)</span> exploitation points using weighted IMSE.
- Select the remainder for exploration.
- Run high-fidelity simulations at new points, update the surrogate, and increment <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p>Repeat steps 2-4 until convergence or computational budget is reached.</p></li>
</ol>
</section>
<section id="remarks">
<h3>Remarks<a class="headerlink" href="#remarks" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>GP-AB focuses GP refinement on regions relevant to the posterior, improving
efficiency over space-filling designs.</p></li>
<li><p>The balance between exploitation and exploration controls robustness: more
exploration is promoted automatically during the initial iterations of GP-AB to reduce the risk of missing important regions.</p></li>
<li><p>Warm-starting TMCMC can further reduce sampling cost when surrogates are accurate.</p></li>
</ul>
<div role="list" class="citation-list">
<div class="citation" id="taflanidis2025" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">Taflanidis2025</a><span class="fn-bracket">]</span></span>
<p>A.A. Taflanidis, B.S. Aakash, S.R. Yi, and J.P. Conte, “Surrogate-aided Bayesian calibration with adaptive learning strategies”, <em>Mechanical Systems and Signal Processing</em>, 237, 113014, 2025. <a class="reference external" href="https://doi.org/10.1016/j.ymssp.2025.113014">https://doi.org/10.1016/j.ymssp.2025.113014</a></p>
</div>
</div>
</section>
</section>
<section id="bayesian-inference-of-hierarchical-models">
<span id="lbluqucsd-hierarchical"></span><h2>Bayesian Inference of Hierarchical Models<a class="headerlink" href="#bayesian-inference-of-hierarchical-models" title="Link to this heading"></a></h2>
<p>Consider for model calibration a dataset consisting of experimental results on test specimens of the same kind. Let <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span>  denote the measured output response of the <span class="math notranslate nohighlight">\(i^{th}\)</span> specimen in the dataset <span class="math notranslate nohighlight">\(\mathbf{Y} = \{\mathbf{y}_1, \mathbf{y}_2, ..., \mathbf{y}_{N_s}\}\)</span> where <span class="math notranslate nohighlight">\(N_s\)</span> designates the number of specimens. Let <span class="math notranslate nohighlight">\(\theta_i\)</span> be the model parameter vector corresponding to the <span class="math notranslate nohighlight">\(i^{th}\)</span> specimen, where <span class="math notranslate nohighlight">\(n_\theta\)</span> denotes the number of model parameters. For the <span class="math notranslate nohighlight">\(i^{th}\)</span> specimen, the output response <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span>  can be viewed as a function of <span class="math notranslate nohighlight">\(\theta_i\)</span>  through the model <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>  and the following measurement equation:</p>
<div class="math notranslate nohighlight" id="equation-measurement-equation">
<span class="eqno">()<a class="headerlink" href="#equation-measurement-equation" title="Link to this equation"></a></span>\[\mathbf{y}_i = \mathbf{h}(\theta_i)  + \mathbf{w}_i\]</div>
<p>where  <span class="math notranslate nohighlight">\(\mathbf{w}_i\)</span> denotes the discrepancy between the response predicted by the model parameterized with <span class="math notranslate nohighlight">\(\theta_i\)</span>, i.e., <span class="math notranslate nohighlight">\(\mathbf{h}(\theta_i)\)</span>, and the experimental output response <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span>; <span class="math notranslate nohighlight">\(\mathbf{w}_i\)</span> is termed the prediction error. With the measurement equation in <a class="reference internal" href="#equation-measurement-equation">()</a>, the sources of real-world uncertainties such as the measurement noise during data collection and model form error are lumped and accounted for in the prediction error (also called noise) term <span class="math notranslate nohighlight">\(\mathbf{w}_i\)</span>  (i.e., <span class="math notranslate nohighlight">\(\mathbf{w}_i\)</span> is also a proxy for model form error). In quoFEM, it is assumed that the prediction error is a zero-mean Gaussian white noise, thus,</p>
<div class="math notranslate nohighlight" id="equation-gaussian-white-noise-prediction-error">
<span class="eqno">()<a class="headerlink" href="#equation-gaussian-white-noise-prediction-error" title="Link to this equation"></a></span>\[\mathbf{w}_i \sim N(0, (\sigma_i^w)^2 \times I_{n_{\mathbf{y}_i} \times n_{\mathbf{y}_i}})\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_i^2\)</span> denotes the variance of the prediction error <span class="math notranslate nohighlight">\(\mathbf{w}_i\)</span>.</p>
<p>With the assumptions made in <a class="reference internal" href="#equation-measurement-equation">()</a> and <a class="reference internal" href="#equation-gaussian-white-noise-prediction-error">()</a>, <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> is Gaussian and centered at the model response <span class="math notranslate nohighlight">\(\mathbf{h}(\theta_i)\)</span>, with a diagonal covariance matrix equal to <span class="math notranslate nohighlight">\(\sigma_i^2 \times \mathbf{I}_{n_{\mathbf{y}_i} \times n_{\mathbf{y}_i}}\)</span> i.e.,</p>
<div class="math notranslate nohighlight" id="equation-likelihood-for-one-dataset">
<span class="eqno">()<a class="headerlink" href="#equation-likelihood-for-one-dataset" title="Link to this equation"></a></span>\[p(\mathbf{y}_i | \theta_i, \sigma_i^2) = N(h(\theta_i), \sigma_i^2 \times \mathbf{I}_{n_{\mathbf{y}_i} \times n_{\mathbf{y}_i}})\]</div>
<p>In the UCSD_UQ engine, hierarchical Bayesian modeling is used to account for specimen-to-specimen variability in the parameter estimates. In the hierarchical Bayesian modeling realm, the parameter vectors <span class="math notranslate nohighlight">\(\theta_1, \theta_2, \ldots, \theta_{N_s}\)</span>  are considered as mutually statistically independent and identically distributed (s.i.i.d) random variables following a PDF <span class="math notranslate nohighlight">\(p(\theta | \eta)\)</span>. This is the PDF of the parent distribution, which represents the aleatory specimen-to-specimen variability.</p>
<p>The objective of hierarchical Bayesian inference is to jointly estimate the model parameters <span class="math notranslate nohighlight">\(\theta_i\)</span> for each specimen i (along with its prediction error variance <span class="math notranslate nohighlight">\(\sigma_i^2\)</span>) as well as the hyperparameters <span class="math notranslate nohighlight">\(\eta\)</span>  of the parent distribution of the set <span class="math notranslate nohighlight">\(\theta_i (i = 1, \ldots, N_s)\)</span>.</p>
<p>Making use of the experimental results for a single specimen, <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span>,  Bayes’ theorem for all parameters to be inferred from this specimen can be written as</p>
<div class="math notranslate nohighlight" id="equation-bayes-rule-for-one-coupon">
<span class="eqno">()<a class="headerlink" href="#equation-bayes-rule-for-one-coupon" title="Link to this equation"></a></span>\[p(\theta_i, \sigma_i^2, \eta | \mathbf{y}_i) \propto p(\mathbf{y}_i | \theta_i, \sigma_i^2, \eta)  p(\theta_i, \sigma_i^2, \eta)\]</div>
<p>The specimen response  <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span> depends solely on the model parameters <span class="math notranslate nohighlight">\(\theta_i\)</span>  and the prediction error variance for that specimen,  <span class="math notranslate nohighlight">\(\sigma_i^2\)</span>, and is independent of the hyperparameters <span class="math notranslate nohighlight">\(\eta\)</span>. Consequently, the conditional PDF <span class="math notranslate nohighlight">\(p(\mathbf{y}_i | \theta_i, \sigma_i^2, \eta)\)</span>  reduces to  <span class="math notranslate nohighlight">\(p(\mathbf{y}_i | \theta_i, \sigma_i^2)\)</span>.</p>
<p>Assuming <span class="math notranslate nohighlight">\(\sigma_i^2\)</span>  to be statistically independent of <span class="math notranslate nohighlight">\(\theta_i\)</span> and <span class="math notranslate nohighlight">\(\eta\)</span> in the joint prior PDF of <span class="math notranslate nohighlight">\(\theta_i\)</span>, <span class="math notranslate nohighlight">\(\sigma_i^2\)</span>, and <span class="math notranslate nohighlight">\(\eta\)</span>, <a class="reference internal" href="#equation-bayes-rule-for-one-coupon">()</a> becomes</p>
<div class="math notranslate nohighlight" id="equation-bayes-rule-for-one-coupon-after-assumptions">
<span class="eqno">()<a class="headerlink" href="#equation-bayes-rule-for-one-coupon-after-assumptions" title="Link to this equation"></a></span>\[p(\theta_i, \sigma_i^2, \eta | \mathbf{y}_i) \propto p(\mathbf{y}_i | \theta_i, \sigma_i^2)  p(\sigma_i^2) p(\theta_i | \eta) p(\eta)\]</div>
<p>In the context of hierarchical Bayesian modeling, the entire experimental dataset from multiple specimens of the same kind, <span class="math notranslate nohighlight">\(\mathbf{Y} = \{\mathbf{y}_1, \mathbf{y}_2, ..., \mathbf{y}_{N_s}\}\)</span>, is considered. The model parameters and the prediction error variances for all specimens are assumed to be mutually statistically independent, while the set of model parameter estimates are assumed to be samples from a parent distribution. The figure <code class="xref std std-numref docutils literal notranslate"><span class="pre">figHierarchicalModel</span></code> shows the structure of the hierarchical model.</p>
<figure class="align-center" id="id4">
<span id="fighierarchicalmodel"></span><img alt="../../../_images/ProbabilisticGraphicalModel.png" src="../../../_images/ProbabilisticGraphicalModel.png" />
<figcaption>
<p><span class="caption-text">Structure of the hierarchical model</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>Under these assumptions, the equation for Bayesian updating of all unknown quantities in the hierarchical model, including the model parameters <span class="math notranslate nohighlight">\(\Theta = \{\theta_1, \theta_2, \ldots, \theta_{N_s}\}\)</span>, measurement noise variances  <span class="math notranslate nohighlight">\(\mathbf{s} = \{\sigma_1^2, \sigma_2^2, \ldots, \sigma_{N_s}^2\}\)</span>, and hyperparameters <span class="math notranslate nohighlight">\(\eta\)</span> is given by</p>
<div class="math notranslate nohighlight" id="equation-bayes-rule-for-hierarchical-model">
<span class="eqno">()<a class="headerlink" href="#equation-bayes-rule-for-hierarchical-model" title="Link to this equation"></a></span>\[p(\Theta, \mathbf{s}, \eta | \mathbf{Y}) \propto p(\eta) \prod_{i=1}^{N_s}[p(\mathbf{y}_i | \theta_i, \sigma_i^2)  p( \sigma_i^2) p(\theta_i | \eta) ]\]</div>
<p>The marginal posterior distribution of the hyperparameters, <span class="math notranslate nohighlight">\(p(\eta | \mathbf{Y})\)</span> , is obtained by marginalizing out <span class="math notranslate nohighlight">\(\mathbf{\Theta}\)</span>  and <span class="math notranslate nohighlight">\(\mathbf{s}\)</span>  from  <span class="math notranslate nohighlight">\(p(\Theta, \mathbf{s}, \eta | \mathbf{Y})\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-marginal-posterior-of-hyperparameters">
<span class="eqno">()<a class="headerlink" href="#equation-marginal-posterior-of-hyperparameters" title="Link to this equation"></a></span>\[p(\eta | \mathbf{Y}) = \int_{\Theta, \mathbf{s}} p(\Theta, \mathbf{s}, \eta | \mathbf{Y}) d\Theta d\mathbf{s}\]</div>
<p>The distribution <span class="math notranslate nohighlight">\(p(\eta | \mathbf{Y})\)</span> describes the epistemic uncertainty in the value of the hyperparameters <span class="math notranslate nohighlight">\(\eta\)</span> due to the finite number of specimens <span class="math notranslate nohighlight">\(N_s\)</span> in the experimental dataset.</p>
<p>In the hierarchical approach, the probability distribution of the model parameters conditioned on the entire measurement dataset, <span class="math notranslate nohighlight">\(p(\theta | \mathbf{Y})\)</span>, is given by</p>
<div class="math notranslate nohighlight" id="equation-posterior-predictive-distribution-of-theta">
<span class="eqno">()<a class="headerlink" href="#equation-posterior-predictive-distribution-of-theta" title="Link to this equation"></a></span>\[p(\theta | \mathbf{Y}) = \int_\eta p(\theta | \eta) p(\eta | \mathbf{Y}) d\eta\]</div>
<p>The conditional distribution <span class="math notranslate nohighlight">\(p(\theta | \eta)\)</span> models the aleatory specimen-to-specimen variability. Therefore, the probability distribution <span class="math notranslate nohighlight">\(p(\theta | \mathbf{Y})\)</span>, referred to as the posterior predictive distribution of the model parameters <span class="math notranslate nohighlight">\(\theta\)</span>, encompasses both the aleatory specimen-to-specimen uncertainty and the epistemic estimation uncertainty (due to the finite number of specimens <span class="math notranslate nohighlight">\(N_s\)</span>  in the experimental dataset and the finite length of the experimental data corresponding to each specimen). It can be utilized for uncertainty quantification and propagation in reliability and risk analyses.</p>
<section id="special-case-normal-population-distribution">
<span id="lbluqucsd-hierarchical-normal"></span><h3>Special Case: Normal Population Distribution<a class="headerlink" href="#special-case-normal-population-distribution" title="Link to this heading"></a></h3>
<p>The hierarchical modeling approach discussed above captures the specimen-to-specimen aleatory variability by modeling the model parameters corresponding to each experiment as a realization from a population distribution. In the UCSD_UQ engine, a multivariate normal distribution <span class="math notranslate nohighlight">\(\theta | \eta \sim N (\mu_\theta, \Sigma_\theta)\)</span> is adopted (i.e., <span class="math notranslate nohighlight">\(\eta = (\mu_\theta, \Sigma_\theta)\)</span>) as:</p>
<div class="math notranslate nohighlight" id="equation-normal-population-distribution">
<span class="eqno">()<a class="headerlink" href="#equation-normal-population-distribution" title="Link to this equation"></a></span>\[\begin{split}\theta_i &amp; \stackrel{s.i.i.d}{\sim} N (\mu_\theta, \Sigma_\theta), \quad i = 1, \ldots, N_s \\
\mu_\theta = \begin{bmatrix} \mu_{\theta_1} \\ \mu_{\theta_2} \\ \vdots \\ \mu_{\theta_{n_\theta}}\end{bmatrix}; \quad \Sigma_\theta &amp;= \begin{bmatrix}    \sigma_{\theta_1}^2 &amp; cov(\theta_1, \theta_2) &amp; \cdots &amp; cov(\theta_1, \theta_{n_\theta}) \\
cov(\theta_2, \theta_1) &amp; \sigma_{\theta_2}^2 &amp; \cdots &amp; cov(\theta_2, \theta_{n_\theta}) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
cov(\theta_{n_\theta}, \theta_1) &amp; cov(\theta_{n_\theta}, \theta_2) &amp; \cdots &amp;  \sigma_{n_\theta}^2 \end{bmatrix}\end{split}\]</div>
<p>In the Bayesian inference process, the hyperparameters characterizing the population probability distribution, the hypermean vector <span class="math notranslate nohighlight">\(\mu_\theta\)</span> and the hypercovariance matrix <span class="math notranslate nohighlight">\(\Sigma_\theta\)</span> are jointly estimated with all the <span class="math notranslate nohighlight">\(\theta_i\)</span> s and <span class="math notranslate nohighlight">\(\sigma_i^2\)</span> s.</p>
<p>So, for this special case, <a class="reference internal" href="#equation-bayes-rule-for-hierarchical-model">()</a> becomes</p>
<div class="math notranslate nohighlight" id="equation-bayes-rule-for-hierarchical-model-with-normal-population">
<span class="eqno">()<a class="headerlink" href="#equation-bayes-rule-for-hierarchical-model-with-normal-population" title="Link to this equation"></a></span>\[p(\Theta, \mathbf{s}, \mu_\theta, \Sigma_\theta | \mathbf{Y}) \propto p(\mu_\theta, \Sigma_\theta) \prod_{i=1}^{N_s}[p(\mathbf{y}_i | \theta_i, \sigma_i^2)  p(\sigma_i^2) p(\theta_i | \mu_\theta, \Sigma_\theta) ]\]</div>
<p>with <span class="math notranslate nohighlight">\(\Theta\)</span>, <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{s}\)</span> as defined above.</p>
<p>In the UCSD_UQ engine, the sampling algorithm operates in the standard normal space i.e.,</p>
<div class="math notranslate nohighlight" id="equation-standard-normal-population-distribution">
<span class="eqno">()<a class="headerlink" href="#equation-standard-normal-population-distribution" title="Link to this equation"></a></span>\[\begin{split}\theta_i  &amp; \stackrel{s.i.i.d}{\sim} N (\mu_\theta, \Sigma_\theta), \quad i = 1, \ldots, N_s \\
\mu_\theta &amp; = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0\end{bmatrix}; \quad \Sigma_\theta = \begin{bmatrix}   1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp;  1 \end{bmatrix}\end{split}\]</div>
<p>and the <a class="reference internal" href="SimCenterUQTechnical.html#lbluqsimtechnical-nataf"><span class="std std-ref">Nataf transform</span></a> is utilized to map the values provided by the sampling algorithm to the physical space of the model parameters before the model is evaluated.</p>
</section>
<section id="sampling-the-posterior-probability-distribution-of-the-parameters-of-the-hierarchical-model">
<span id="lbluqucsd-hierarchical-sampling-algorithm"></span><h3>Sampling the Posterior Probability Distribution of the Parameters of the Hierarchical Model<a class="headerlink" href="#sampling-the-posterior-probability-distribution-of-the-parameters-of-the-hierarchical-model" title="Link to this heading"></a></h3>
<p>Due to the high dimensionality (<span class="math notranslate nohighlight">\(n_\theta \times n_s + n_s + n_\theta + n_\theta \times \frac{(n_\theta+1)}{2}\)</span>, corresponding to the dimensions of the <span class="math notranslate nohighlight">\(\theta_i\)</span> of each dataset, <span class="math notranslate nohighlight">\(\sigma_i^2\)</span> for each dataset, <span class="math notranslate nohighlight">\(\mu_\theta\)</span> and <span class="math notranslate nohighlight">\(\Sigma_\theta\)</span>, respectively) of the posterior joint PDF shown in <a class="reference internal" href="#equation-bayes-rule-for-hierarchical-model-with-normal-population">()</a>, the Metropolis within Gibbs algorithm is used to generate samples from the posterior probability distribution. To do this, conditional posterior distributions are derived for blocks of parameters from the joint posterior distribution <a class="reference internal" href="#equation-bayes-rule-for-hierarchical-model-with-normal-population">()</a> of all the parameters of the hierarchical model. The Gibbs sampler generates values from these conditional posterior distributions iteratively. The conditional posterior distributions are lower dimensional than the joint distribution. The assumptions made next result in a few of the conditional posterior distributions being of a form that can be easily sampled from, thereby making it feasible to draw samples from the high-dimensional joint posterior PDF of the hierarchical model.</p>
<p>The prior distributions for each <span class="math notranslate nohighlight">\(\sigma_i^2\)</span> are selected as the inverse gamma (IG) distribution:</p>
<div class="math notranslate nohighlight" id="equation-conjugate-prior-pdf-for-sigma-i">
<span class="eqno">()<a class="headerlink" href="#equation-conjugate-prior-pdf-for-sigma-i" title="Link to this equation"></a></span>\[p(\sigma_i^2) = IG(\alpha_0, \beta_0)\]</div>
<p>The prior probability distribution for <span class="math notranslate nohighlight">\((\mu_\theta, \Sigma_\theta)\)</span> is chosen to be the normal-inverse-Wishart (NIW) distribution:</p>
<div class="math notranslate nohighlight" id="equation-conjugate-prior-pdf-for-hyperparameters">
<span class="eqno">()<a class="headerlink" href="#equation-conjugate-prior-pdf-for-hyperparameters" title="Link to this equation"></a></span>\[p(\mu_\theta, \Sigma_\theta) = N(\mu_\theta | \mu_0, \frac{\Sigma_\theta}{\nu_0}) IW(\Sigma_\theta | \Sigma_0, m_0)\]</div>
<p>With these assumptions, the conditional posterior distributions are the following:</p>
<div class="math notranslate nohighlight" id="equation-conditional-posterior-distributions">
<span class="eqno">()<a class="headerlink" href="#equation-conditional-posterior-distributions" title="Link to this equation"></a></span>\[\begin{split}p(\Sigma_\theta &amp;| \mathbf{Y},  \Theta, \mathbf{s}, \mu_\theta) \propto IW(\Sigma_\theta | \Sigma_n, m_n) \\
p(\mu_\theta &amp;| \mathbf{Y},  \Theta, \mathbf{s}, \Sigma_\theta) \propto N(\mu_\theta | \mu_n, \frac{\Sigma_\theta}{\nu_n}) \\
p(\sigma_i^2 &amp;| \mathbf{Y}, \Theta, \mu_\theta, \Sigma_\theta) \propto IG(\alpha_{n_i}, \beta_{n_i})\\
p(\theta_i &amp;| \mathbf{Y}, \mathbf{s}, \mu_\theta, \Sigma_\theta ) \propto p(\mathbf{y}_i | \theta_i, \sigma_i^2)   p(\theta_i | \mu_\theta, \Sigma_\theta)\end{split}\]</div>
<p>where:</p>
<div class="math notranslate nohighlight" id="equation-common-technical-manual-desktop-ucsduqtechnical-8">
<span class="eqno">()<a class="headerlink" href="#equation-common-technical-manual-desktop-ucsduqtechnical-8" title="Link to this equation"></a></span>\[\begin{split}\nu_n &amp;= \nu_0 + N_s \\
\mu_n &amp;= \frac{(\mu_0 * \lambda_0 + N_s * \bar{\theta})}{\nu_n} \\
\Sigma_n &amp;= \Sigma_0 + S + \frac{\nu_0 * N_s}{\nu_n} \times ((\bar{\theta} - \mu_0)(\bar{\theta} - \mu_0)^T) \\
m_n &amp;= m_0 + N_s \\
\alpha_{n_i} &amp;= \alpha_0 + \frac{n_{\mathbf{y}_i}}{2}\\
\beta_{n_i} &amp;= \beta_0 + \frac{{sse}_i}{2}\end{split}\]</div>
<p>and,</p>
<div class="math notranslate nohighlight" id="equation-common-technical-manual-desktop-ucsduqtechnical-9">
<span class="eqno">()<a class="headerlink" href="#equation-common-technical-manual-desktop-ucsduqtechnical-9" title="Link to this equation"></a></span>\[\begin{split}\bar{\theta} &amp;= \frac{1}{N_s}\sum_{i=1}^{N_s} \theta_i \\
S &amp;= \sum_{i=1}^{N_s} {(\bar{\theta} - \mu_0)(\bar{\theta} - \mu_0)^T} \\
{sse}_i &amp;= \sum_{k=1}^{n_{\mathbf{y}_i}}(\mathbf{y}_i^{(k)} - \mathbf{h}(\theta_i)^{(k)})^2\end{split}\]</div>
<p>The Metropolis within Gibbs sampler generates values from the posterior joint PDF shown in <a class="reference internal" href="#equation-bayes-rule-for-hierarchical-model-with-normal-population">()</a> using the posterior conditional distributions <a class="reference internal" href="#equation-conditional-posterior-distributions">()</a> through the following steps:</p>
<ol class="arabic">
<li><p>Select appropriate initial values for each of the <span class="math notranslate nohighlight">\(\theta_i\)</span>.</p></li>
<li><p>Based on the <span class="math notranslate nohighlight">\(k^{th}\)</span> sample value, the <span class="math notranslate nohighlight">\((k+1)^{th}\)</span> sample value is obtained using the following steps:</p>
<ol class="arabic">
<li><p>Generate a sample value of <span class="math notranslate nohighlight">\(\Sigma_{\theta}^{(k+1)}\)</span> from the conditional posterior distribution <span class="math notranslate nohighlight">\(IW(\Sigma_n, m_n)\)</span></p></li>
<li><p>Generate a sample value of <span class="math notranslate nohighlight">\(\mu_\theta^{(k+1)}\)</span> from the conditional posterior distribution <span class="math notranslate nohighlight">\(N(\mu_n, \frac{\Sigma_\theta}{\nu_n})\)</span></p></li>
<li><p>For each dataset, generate independently a sample value of <span class="math notranslate nohighlight">\(\sigma_i^{2(k+1)}\)</span> from the conditional posterior distribution of each <span class="math notranslate nohighlight">\(\sigma_i^2\)</span>: <span class="math notranslate nohighlight">\(IG(\alpha_n, \beta_n)\)</span></p></li>
<li><p>For each dataset, generate a sample value of the model parameters <span class="math notranslate nohighlight">\(\theta_i\)</span> from the conditional posterior <span class="math notranslate nohighlight">\(p(\mathbf{y}_i | \theta_i, \sigma_i^2)   p(\theta_i | \mu_\theta, \Sigma_\theta)\)</span> with a Metropolis-Hastings step as follows:</p>
<ol class="loweralpha simple">
<li><p>Generate a candidate sample <span class="math notranslate nohighlight">\(\theta_{i(c)}\)</span> from a local random walk proposal density <span class="math notranslate nohighlight">\(\theta_{i(c)} \sim N(\theta_i^{(k)}, \Sigma_i^{(k)})\)</span> where <span class="math notranslate nohighlight">\(\Sigma_i^{(k)}\)</span> is the proposal covariance matrix of the random walk.</p></li>
<li><p>Calculate the acceptance ratio</p></li>
</ol>
<div class="math notranslate nohighlight" id="equation-common-technical-manual-desktop-ucsduqtechnical-10">
<span class="eqno">()<a class="headerlink" href="#equation-common-technical-manual-desktop-ucsduqtechnical-10" title="Link to this equation"></a></span>\[a_c = \frac{p(\mathbf{y}_i | \theta_{i(c)}, \sigma_i^{2(k+1)}) p(\theta_{i(c)} | \mu_\theta^{(k+1)}, \Sigma_{\theta}^{(k+1)})}{p(\mathbf{y}_i | \theta_i^{(k)}, \sigma_i^{2(k+1)})   p(\theta_i^{(k)}| \mu_\theta^{(k+1)}, \Sigma_{\theta}^{(k+1)})}\]</div>
<ol class="loweralpha simple" start="3">
<li><p>Generate a random value <span class="math notranslate nohighlight">\(u\)</span> from a uniform distribution on [0, 1] and set</p></li>
</ol>
<div class="math notranslate nohighlight" id="equation-metropolis-hastings-criteria">
<span class="eqno">()<a class="headerlink" href="#equation-metropolis-hastings-criteria" title="Link to this equation"></a></span>\[\begin{split}\theta_i^{(k+1)} = \quad &amp;\theta_{i(c)} \quad \text{if} \quad u &lt; a_c \\
&amp;\theta_i^{(k)} \quad \text{otherwise}\end{split}\]</div>
</li>
</ol>
</li>
</ol>
<p>After an initial burn-in period, the sample values generated by this algorithm converge to the target density. The proposal covariance matrix of the random walk <span class="math notranslate nohighlight">\(\Sigma_i^{(k)}\)</span> is selected to facilitate proper mixing of the random walks used to generate sample values of the <span class="math notranslate nohighlight">\(\theta_i\)</span>. Within the adaptation duration, a scaled version of the covariance matrix of the sample values of the last <span class="math notranslate nohighlight">\(N_{cov}\)</span> samples (defined as the adaptation frequency in the UCSD_UQ engine) is used, to keep the acceptance rate in the range of 0.2 to 0.5.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, The Regents of the University of California.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-158130480-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-158130480-2', {
          'anonymize_ip': false,
      });
    </script>  

  <style>
         .wy-nav-content { max-width: none; }
  </style>

<script>
    /*
    let selectedFilters = [];
    const images  = document.getElementsByClassName("gallery-item");
    const filters = [...document.querySelectorAll('.filter select')];
    const toggles = [...document.querySelectorAll('.filter input')];

    var show = function (elem) {
        elem.style.display = 'block';
    };
    var hide = function (elem) {
        elem.style.display = 'none';
    };
    var toggleFilter =  function(el,elid) {
        const filter = document.getElementById(elid);
        filter.disabled = !el.checked;
       
        if ("createEvent" in document) {
            var evt = document.createEvent("HTMLEvents");
            evt.initEvent("change", false, true);
            filter.dispatchEvent(evt);
        }
        else
            filter.fireEvent("change");
    };
    
    for (const filter of filters) {
        filter.addEventListener('change', function(event) {
            selectedFilters = filters.map(filter => filter.disabled ? '' : filter.value).filter(Boolean);
            console.log(selectedFilters);
            for (const image of images) {
                if (selectedFilters.every(filter => image.classList.contains(filter))) {
                    show(image);
                }
                else {hide(image)};
            };
        })
    };
    */
</script>


</body>
</html>